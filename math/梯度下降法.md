#梯度下降法（Gradient Descent）介绍
===

*1.梯度下降法  
**是一种基于搜索的最优化方法**  
**作用**：最小化一个损失函数  
**梯度上升法**：最大化一个效用函数

给定目标函数分f(x)和初始点x0  





导数和偏导的区别：导数和偏导，偏导是因为有很多未知数，每次只能对其中一个求导，所以叫偏导



**低维空间：**
![](https://ws2.sinaimg.cn/large/0069RVTdly1fuxbhpdhxmj30vf0hwgoi.jpg)


**高维空间：**
![](https://ws1.sinaimg.cn/large/0069RVTdly1fuxbialzmaj30xa0ibdjm.jpg)
梯度代表方向，代表J增大最快的方向


**线性回归使用梯度下降**  
目标函数：
![](https://ws3.sinaimg.cn/large/0069RVTdly1fuxbrwj2i5j30gu07b3zi.jpg)



![](https://ws4.sinaimg.cn/large/0069RVTdly1fuxbx22gdkj30v90f3n1c.jpg)

化简后：
![](https://ws2.sinaimg.cn/large/0069RVTdly1fuxbz9r6hdj30wr0i4wjl.jpg)

继续化简
![](https://ws2.sinaimg.cn/large/0069RVTdly1fuxge1n3uaj30xk0el77p.jpg)







*2.考虑冲量的梯度下降法  







