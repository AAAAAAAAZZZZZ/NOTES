# 两个月刷完机器学习（包含深度学习）600题。（题目来源于BAT等厂面试题）  每天10道。    
## 第一天:    
1、下列时间序列模型中,哪一个模型可以较好地拟合波动性的分析和预测。    
A AR模型    
B MA模型    
C ARMA模型    
D GARCH模型        
解析：AR auto regressive model AR模型是一种线性预测
MA模型(moving average model)滑动平均模型，其中使用趋势移动平均法建立直线趋势的预测模型。
ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐。    
GARCH模型称为广义ARCH模型，是ARCH模型的拓展，由Bollerslev(1986)发展起来的。它是ARCH模型的推广。GARCH(p,0)模型，相当于ARCH(p)模型。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测。    
正确答案 D    
2、以下说法中错误的是（）    
A  SVM对噪声（如来自其他分部的噪声样本）具备鲁棒性    
B  在adaboost算法中，所有被分错样本的权重更新比例不相同     
C  boosting和bagging都是组合多个分类器投票的方法，二者都是根据单个分类器的正确率确定其权重   
D  给定n个数据点，如果其中一半用于训练，一半用户测试，则训练误差和测试误差之间的差别会随着n的增加而减少的    
解析：    
A   软间隔分类器对噪声是有鲁棒性的    
B   具体说来，整个Adaboost 迭代算法就3步：    
初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。
训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。    
将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。    
C    boosting是根据分类器正确率确定权重，bagging不是。    
Bagging即套袋法，其算法过程如下：    
A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）    
B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）   
C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）
Boosting其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。    
关于Boosting的两个核心问题：    
1）在每一轮如何改变训练数据的权值或概率分布？
通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。    
2）通过什么方式来组合弱分类器？    
通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。
而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。    
D    训练集变大会提高模型鲁棒性。    
正确答案C    
3、你正在使用带有 L1 正则化的 logistic 回归做二分类，其中 C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。当你把 C 值从 0 增加至非常大的值时，下面哪个选项是正确的？    
![image](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1514624605_280.png)
A 第一个 w2 成了 0，接着 w1 也成了 0    
B 第一个 w1 成了 0，接着 w2 也成了 0    
C w1 和 w2 同时成了 0    
D 即使在 C 成为大值之后，w1 和 w2 都不能成 0    
解析：L1正则化的函数如图，所以w1和w2可以为0。同时w1和w2是对称的，不会导致一个为0另一个不为0的状态。    
正确答案D    
4、在 k-均值算法中，以下哪个选项可用于获得全局最小？    
A 尝试为不同的质心（centroid）初始化运行算法    
B 整迭代的次数   
C 找到集群的最佳数量    
D 以上所有   
解析：所有都可以用来调试以找到全局最小。    
正确答案D    
5、假设你使用 log-loss 函数作为评估标准。下面这些选项，哪些是对作为评估标准的 log-loss 的正确解释。    
A 如果一个分类器对不正确的分类很自信，log-loss 会严重的批评它   
B 对一个特别的观察而言，分类器为正确的类别分配非常小的概率，然后对 log-loss 的相应分布会非常大    
C log-loss 越低，模型越好   
D 以上都是   
解析：损失函数总结(https://blog.csdn.net/ZHANG781068447/article/details/82752598)
正确答案D
6、下面哪个选项中哪一项属于确定性算法？
A  PCA   
B  K-Means   
C  以上都不是   
解析：确定性算法表明在不同运行中，算法输出并不会改变。如果我们再一次运行算法，PCA 会得出相同的结果，而 k-means 不会。    
正确答案：A    
7、两个变量的 Pearson 相关性系数为零，但这两个变量的值同样可以相关。这句描述是正确还是错误？   
A 正确   
B 错误    
解析：
答案为（A）：Pearson相关系数只能衡量线性相关性，但无法衡量非线性关系。如y=x^2，x和y有很强的非线性关系。

8、下面哪个/些超参数的增加可能会造成随机森林数据过拟合？        
A 树的数量    
B 树的深度    
C 学习速率    
解析：   
答案为（B）：通常情况下，我们增加树的深度有可能会造成模型过拟合。学习速率并不是随机森林的超参数。增加树的数量可能会造成欠拟合。   
9、下列哪个不属于常用的文本分类的特征选择算法？    
A 卡方检验值    
B 互信息    
C 信息增益    
D 主成分分析    
解析：    
答案D    
常采用特征选择方法。常见的六种特征选择方法：    
1）DF(Document Frequency) 文档频率             
DF:统计特征词出现的文档数量，用来衡量某个特征词的重要性    
2）MI(Mutual Information) 互信息法    
互信息法用于衡量特征词与文档类别直接的信息量。
如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向"低频"的特征词。    
相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。    
3）(Information Gain) 信息增益法    
通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。    
4）CHI(Chi-square) 卡方检验法    
利用了统计学中的"假设检验"的基本思想：首先假设特征词与类别直接是不相关的    
如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。    
5）WLLR(Weighted Log Likelihood Ration)加权对数似然    
6）WFO（Weighted Frequency and Odds）加权频率和可能性    
10、机器学习中做特征选择时，可能用到的方法有？    
A 卡方    
B 信息增益
C 平均互信息    
D 期望交叉熵    
E 以上都有    
正确答案是：E    
