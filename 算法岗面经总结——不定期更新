目录：
头条
贝克找房：
京东
拼多多：
大疆：
快手：
百度
链家
阿里菜鸟-机器学习
美丽联合
美团点评
搜狗
三星研究所
猿辅导
招银网络
海康威视
农行软开
华为：
1.项目。。。各种扣
2.说一下卷积神经网络（AlexNet,GoogleNet,VGG,ResNet,DenseNet）
3.全连接层和卷积层连接方式有什么区别，有什么作用
4.Inception模块的作用
5.神经网络怎么用Tensorflow封装成Java可以调用的API，原理是什么？
6.各种个人问题职业规划

网易云音乐：
1. 一面（技术一面） 自我介绍 介绍项目 用了哪些特征？ 
平时是怎么学习的？ 
bagging和boosting有什么区别？
 bagging和boosting和树有哪些组合方式？
 XGBoost和GBDT有什么区别，推导XGBoost。（
会手推算法很重要，我自己准备了感知机，LR，朴素贝叶斯，最大熵，EM算法，SVM，XGBoost，牛顿，拟牛顿，HMM里的前向后向算法、维特比算法，BP算法等算法的手推，除了BP算法和XGB外，这些算法在李航老师的《统计学习方法》里都找得到，最大熵步骤最多。） 
手写代码：字符串反转 讲一讲K-Means算法 你有什么想问的？ 
2.二面（技术二面） 自我介绍 介绍项目 用了哪几组特征？ 
贝叶斯调参是怎样的？（因为我比赛中用过贝叶斯调参，所以会问到） 知不知道排名第一的大佬是怎么做的？（这个问题很多面试官都会问，参加过比赛的同学可以去研究一下前排大佬的做法，这题答得好会加分很多）
 为什么不实习？
 L1正则和L2正则有什么区别？ 
CNN中如果想用一个卷积提取局部特征和全局特征，该怎么做？（不会，这题有哪位大佬知道答案嘛？） 
看过哪些书 
牛顿法是怎样的？写一下过程。（一阶函数和多阶函数的牛顿法） 
推导SVM 自己想一个非线性分类问题，设计其核函数
 手写代码：m*n，从左上到右下最少的步数。（用dp做，比较简单。这里我做了一个优化，空间复杂度从O(m*n)降低到O(min(m,n))） 
3.三面（hr面） 自我介绍（被吐槽说是不是背的2333） 
觉得比赛中提分大的是哪个步骤，你是怎么做的？为什么别人不这样做？（当时我就被问懵了，我也不知道别人为啥不这样做啊。还好及时反应过来，反过来回答，说自己为什么会想到这么做，顺便不要脸地夸了自己一波） 
家里情况？ 面试过哪几家？为什么那么晚投？（这里有坑，我说前面投的是小厂，试试水。然后被反问网易算不算小厂？赶紧说不算不算，然后改口不是不是。）
 你为什么转行？ 为什么选择计算机？ 为什么选择算法？（这三问简直是对灵魂的拷问） 
室友对你的转行怎么看？你晚上写代码会不会影响到他们的休息？
 想去哪个城市工作？父母对工作城市有要求吗？ 对网易云音乐有何建议？ 你有什么想问的？
4. 四面（总监面） 面试官气场很足，但是很温和。
介绍项目 贝叶斯调参是怎样的？ 为什么转行？ 比赛为什么不组队？ 项目侧重点，特征工程是怎么做的？ 学过哪些课程？有没有学操作系统或者是编译原理？ 有没有实现梯度下降或者拟牛顿法？ LightGBM和XGBoost有什么区别？实际使用过程中对两者的感受是怎样的？从原理解释下原因？ 会L-BFGS吗？L-BFGS哪一步很巧妙？是看过论文吗？ 你有什么想问的？ 

网易有道：
1、自我介绍
2、项目介绍
3、核函数有哪些？如何选择？核函数正定的意义？
4、解释维度灾难，如何解决？
5、svm如何处理高维特征？
6、knn如何处理高维特征？
7、kmeans如何处理高维特征？kmeans如何处理异常点？
8、问了一道概率题，代入泊松分布公式即可
最后问有什么要问他的，我问面试官你们的数据有多少维（问这么多高维问题），答曰1000维。。

网易游戏：
一面：
Wod2vec公式
FM公式
LR公式
softmax公式
看过的论文，讨论论文
代码题：股票最大值。

二面：
神经网络为啥用交叉熵。
交叉熵公式
注意力公式

论文flow情况
代码题：
Kmeans
快排

三面：
编辑距离（没写出来。挂）
Python 多进程
Python 锁
面试官说我没有insight,公式能力差，离research差很多，工程能力也很差，适合做数据挖掘。
面的人工智能研究员nlp，真的难受，很多原理是懂，但是真的写不出公式。面试官说写不出公式还是理解的不够透彻。也许没论文、双非的我真的没法高攀research这种岗位，老老实实地搞数据挖掘真的不甘心。
1. 一上来，面试官先做了好几分钟的自我介绍和互娱介绍（第一次这种面试体验，还是蛮不错的）；
2. 简单进行自我介绍；
3. 讲一下简历上最新的一个项目；
4. RF和GBDT的区别；RF怎么解决的过拟合问题；
5. 决策树的分裂策略：ID3，C4.5，Gini指数，选一个讲一下；
6. 熵了解吗？
7. 讲一下XGBoost；
8. 正则化项了解吗？
9. 优化算法： 牛顿法和拟牛顿法，过程？（这个不会，李航老师的统计机器学习附录有详细讲解）；
10. 熟悉深度学习吗?
11. 讲一下word2vec；
12 算法题： 找出数组中第K大的数。有几种方法？时间复杂度分别是多少？
头条
头条 （视频面）
一面 ：二面： 3面：
1  讲项目（每面都问，问细节）。
2  code，拟合带噪音线性数据，输出直线参数，
3 升级一下，加大噪音量和噪音距离（code）。
4 GBDT思想，算法过程。 XGBOOST
5 给一个二叉树，输出所有完全一样的（重复的）子树。 要求O（N）（code）
6 场景题，电梯算法设计，机器学习怎么搞
7 新闻推荐如果用强化学习，怎么设计。
8 听音乐，怎么推荐，10亿首，怎么设计方案（K-D树以及实现）。
。。还有一些小问题。。。。

第4面（加面）：
1 问实习经历
2 一个圆上随机三点，求形成锐角三角形概率
3 一个无序数字序列，每次只能左旋操作3个数，求要求有序下，证明能否通过有限次数能否有序。

一面（面试官长得像胡歌）（50min）
自我介绍
编程题：n点，最多共线有多少个。
接上问，复杂度是多少？（我的是O(n^2*lgn)）问能否优化到O(n^2)？
讲项目
牛顿法是怎样的，和梯度下降法优缺点比较
你有什么问题

二面（42min）
自我介绍
讲项目（用了哪些特征，为什么用这些特征）
第一名大佬是怎么做的？
CNN了解到了什么程度，用过CNN吗？
平时用什么语言，会C/C++吗？
朴素贝叶斯公式
牛顿法是怎样的
编程题：二叉树，逐层反向遍历

三面（60min）
讲项目
有没有了解其他算法(FM..)
工作地点
编程题：
将只含有AB两种字符的字符串重排，要求1.前面B是每4个最多出现1次，2.B只能往后移动，3.尾部字符串可以违反规则1.

一面：大约30min
自我介绍
实习项目介绍，问得很细
算法题，旋转数组求中位数，听完问题就下笔写，几分钟写完，用遍历做的。问能不能优化，用二分再写了一遍
要求介绍LR或GBDT，讲了LR，由来推导都讲了一遍

二面：大约35-40min
自我介绍
依然是实习项目，问的细，里面有个部分需要用到word embedding，实现的时候是用的别的组的工具，所以追问了一下，如何自己实现，按着nlp的思路讲了一遍，包括分词，求主体一类的
算法题：
求树的最大深度，迅速写好
求树的最大路径，即a点到b点经过的结点数，迅速写好
还有一道算法题忘记了

（二面面完，走回休息区，屁股都没坐下就叫去三面了……也是服气）

三面：1h以上
之前参加的数据挖掘比赛，讲了一下如何挖特征的，听到一半说不用听了
问paper，也是讲了一半说不听了
问 nlp research方向的项目，从背景开始讲，问“那你告诉我，这个idea是谁哪一年提出来的，后面有谁和哪些实验室一直在follow”，不知道（无语，又不是AI岗，问这些）然后问“现在给你100篇新领域的paper，你怎么从里面挑出你要看的”，大概说了一下，先看综述，然后筛选方向，然后用abstract和会议来筛一下。
然后问了一下在做的idea，亮点是什么，提到attention的时候问query key value各是啥（其实是attention机制里的概念），我的项目里的attention很简单，不涉及这么复杂的概念，所以最后把自己用的方法讲了一下，但面试官坚持要问query key value，没答上来
（差不多到这儿已经懵逼了，导致后面全程都答得不好）
算法题，一个无序数组，求最长递增子序列的长度，说可以不写code讲idea，答这应该是个dp问题，然后用最基本的O（N^2）算法讲了一遍，问，能不能优化，我说了一下优化的方向，即优化遇到断点回溯的查询，然后在思考优化方式的过程中，一会儿提醒我这样，一会儿提醒我那样，加上旁边一个面试官一直大声疯狂diss面试者，完全被搞糊涂了，最后作罢（如果实现刷过这个题，应该不会这么惨，也是自己的问题，准备不够）
最后又问了一道算法题，一个链表按k个为一组进行反转，很快地写完了，看了代码，说我的思路太绕了（头节点的判断，我不认为这个判断很奇怪...），然后指出有个地方忘写了引用，这个飞快反应过来了
然后就是瞎聊，最后跟面试官说了一句感觉自己表现不太好

这是第二次面头条
上一次去头条面试大约是一年前？
当时是面算法实习生
一面手撕三道算法题，基本都是leetcode，飞快写完
二面面试官全程只问数学题？比如病态矩阵的性质，奇异值分解的一些应用之类，懵逼了
三面面试官更奇怪，让你讲knn，讲完了不满意，非要用一句话概括，我就讲了一个带逗号的句话，不满意，说你说的太长了，我不要听那么长.... 大约全程就这样...

一面：
一直在问faster rcnn，faster rcnn RPN流程，anchor选的太大或太小有什么影响，正负样本选取的时候为什么用0.7和0.3的超参，与nms使用0.3的超参是否有关，smooth l1损失为什么不用l1，为什么不用l2，w，h回归的时候为什么要用log，x，y回归的时候为什么用除法等等。。
手写nms
二面：
问项目的时候说我做的太low。。。
C++虚函数，vector底层如何实现插入的时候不改变内存空间
两个题：
x,y属于[0,1]的均匀分布，求max（x,y）的期望
一个递归的题，面试官表达有误刚开始理解错了，这个题太复杂了，就不写了，漏了一个条件，然后就挂了。。
贝克找房：
一面：
全程聊项目，面试官看起来挺年轻，但是我是做深度学习图像的，感觉他不是很了解，他是做3D视觉的应该，总之我会的他不熟，他熟的我不会……有点尴尬……把项目讲了一遍就结束了
二面：
先讲项目，然后问了几道题，不一定记得全：
卷积层和全连接层的区别
L0 L1 L2
防止过拟合方法
数据增强方法
python中tuple和list的区别
复杂度了解吗，二分查找复杂度
快排最好情况的复杂度
手撕代码：二分查找【居然写错了😂丢人……】
问我了解堆和栈吗，答不了解堆，栈还可以
用栈结构实现O(1)时间复杂度找到栈内的最大元素，如果有很多重复元素时怎么改进
还问了PCA原理，答得不怎么好
于是问了个特征值和特征向量的含义，答曰忘了【其实根本没会过😭】
如果要给你们学校的宿舍安排外卖员，需要雇佣几个外卖员才够，如果没有任何数据作参考怎么办？不知道是不是答到面试官想要的点上了
别的想不起来了
三面HR：
都是常规题
自我介绍；觉得自己有哪些优点，举例说明，有哪些缺点，举例说明，如何克服这些缺点；说一个自己参与度最高的项目，如果复盘重新做你会有哪些改进；还面试过哪些公司；找工作看重哪些方面，对企业工作环境等有哪些期望；如何提升自己能力，期望薪酬，职业规划等等……

京东
京东的两面交叉面，写一些能够想起来的面筋，攒波人品
一面：
1、实习做的特征相关的工作，问的如何判断特征的有效性
2、FM模型的具体公式，FFM在此基础上有什么改进，如何确定每一个特征所属的field
3、LGB、XGB的区别和联系，并行是如何并行的
4、写一下Gini系数、信息增益、信息增益率的公式
5、讲一下一个文本相关的比赛，一些细节
6、写一下LSTM的结构和前向的传播公式
7、SGD和Adam的区别和联系
8、代码题：快排
二面：
1、同样的问题，如何判断特征有效性
2、讲一下XGB的原理，推一下公式
3、用L-BFGS来推导一下logistic regression的迭代公式
4、XGB的VC维是多少，决策树呢
5、推一下word2vec的原理
6、讲一下阿里妈妈的一个比赛的细节
7、代码题：一个数组里面，每K个数是一个递增的有序数组，将整个数组排序
8、其他的问题大概记不住了，反正这一面的问题基本上都是拉闸

一面：
基本上就是问项目和比赛经历
讲一下unet和deeplabv2的流程，顺便问了下deeplabv3，crf是否了解
faster rcnn 流程以及RPN的具体过程
白纸写代码部分：二叉树前序遍历，手写nms
二面：
也差不多是问的项目和比赛，具体忘记了。。
2017-08-11-京东广告数据部-机器学习-内推1面-电话
1、自我介绍
2、说一下进程和线程
说一大堆，再就说之间的区别
3、线程安全的理解
4、有哪些线程安全的函数
5、数据库中主键、索引和外键。以及作用
一个表可以没有主键，可以有索引
6、说项目
7、Spark原理
8、Spark是多线程模式，怎么退化为多进程模式。
在每个executor core设置为1，即每个executor是单线程的。
9、撸代码。实现一个java迭代器
数据：
int[][] data = new int[][] {
null,
new int[] {1,2,3,4},
new int[] {},
null,
new int[] {5,6,7},
new int[] {8},
null,
};
要求：遍历是跳过NULL。依次遍历每个元素：1,2,3,4,5,6,7,8
提示：
hasNext里面不应该改变迭代器内部状态，hashNext只判断
next返回值，并且指向下一个有效元素。

P.S. 面试官很忙，在我写代码的时候。还在跟另一个候选人约时间~~~
-----------------------------------------------------------------------------------------------------------
2017-08-11-京东广告数据部-机器学习-内推2面-电话 一面、二面连着玩~~~
1、自我介绍
2、对于机器学习你都学了哪些？讲一个印象深的
说了SVM原理，拉格朗日法，对偶问题，以及好处。
3、SVM怎么防止过拟合
说了SVM里面的松弛变量。不知道对不对
4、我主动出击，有另一大类算法决策树，说不管是LR还是SVM都不能直观的感受到决策依据。而决策树易于理解，能够直观的感受到决策依据。
说了划分依据：信息增益（说了信息熵的来源，等概率时熵最大）、信息增益率、基尼系数。
说了划分方法（基于信息增益的）
说了C4.5比较ID3的优点。
5、决策树如何防止过拟合
剪枝，前剪枝和后剪枝。说了REP剪枝。C4.5是悲观剪枝
6、项目没问，说从上位面试官了解了。
7、撸代码
求连续子数组最大乘积，还让考虑边界问题（最后问了：连乘有可能导致溢出，存不下了）



拼多多：
一面：
1、自我介绍和讲项目
2、推导一下GBDT的公式、LGB和XGB的区别
3、推导SVM
4、画出LSTM的结构图
5、代码：最长上升子序列
二面：
二面基本上在聊天，就写了一个代码，实现一个KMeans聚类的一个类
然后HR面就是扯一些杂事
2017-08-29-拼多多-算法-2面-内推-电话
1、自我介绍
2、将项目
3、说SVM
4、好像还说了spark原理
5、电话中断，面试官线上有BUG，去改BUG了。。。
6、10分钟后电话来了
7、我主动说：我给您说一下决策树方面的吧
8、面试官说：不用了，来道题。。。
9、一个矩阵都是0,1 且每一行，0都在1前面。求1个数最多的那一行的序号

2017-08-15-拼多多-算法-内推1面-电话

1、自我介绍
2、介绍项目
3、项目延展题：电商搜索框，每天有500W的搜索query。针对新来的一个query，给出和它最相似的100个query。
如果用RNN分类模型表征，那么向量不应该用最后一层的分类特征。应该用倒数第二层的更纯的特征。
现在假设500W的query已经是向量了。如何和这一个query比较。全部算距离不行，开销太大。
应该怎么办？？？
4、K-means聚类个数选择，做什么样的试验来确定K
5、两个4G的文件（每个文件可能有重复），里面全都是数字。现有内存1G，求这两个文件的交集。
2个4G的文件，分别hash成10个子文件，一个400M。
把一个子文件存储到hash表中，作为key。遍历另一个文件，看这个数字是否存在于刚才的hash表中。存在即可输出。



大疆：
1、两个小车，走一步能量消耗1，方向为1向右，-1为向左，首先输入路途长度，然后输入两行，每行第一个为小车的能量，第二个位小车起始位置，第三个为方向。求几个小车可以走出去？
2、一共N种花，插花需要每次选M种，每种R支。第二行输入每种花个数，求最多有多少种插花方法。
3、输入初始位置和结束位置，以及二维数组的大小，与其中的元素，为0可以走，为1，其上下左右不能走，如果为2，则该位置的上两个，下两个，左两个，右两个不能走。以此类推，求最短路径？
4、H的水桶，注水速度X，h处有洞，流水速度Y，S秒以后水深（四舍五入）。

1、考察 L1 和 L2 正则化的区别
L0 范数：向量中非0元素的个数。
L1 范数 (Lasso Regularization)：向量中各个元素绝对值的和。
L2 范数(Ridge Regression)：向量中各元素平方和再求平方根。
L0 范数和 L1 范数都能够达到使参数稀疏的目的，但 L0 范数更难优化求解，L1 范数是 L0 范数的最优凸近似，而且它比 L0 范数要容易优化求解。
L2 范数不但可以防止过拟合，提高模型的泛化能力，还可以让我们的优化求解变得稳定和快速。L2 范数对大数和 outlier 更敏感！
2、考察 SVM 决策边界
四个点坐标为(1,1)，(1,0)，(-1,-1)，(-1,0)，用 SVM 分类的决策边界是
A. y = x
B. x = 0
C. y = -x
D. y = 0

填空题
1.经过下列卷积操作后，3×3 conv -> 3×3 conv -> 2×2 maxpool -> 3×3 conv，卷积步长为 1，没有填充，输出神经元的感受野是多大？
2.100×100×3，3×3 卷积核，输出是 50×50×10，算进行了多少次乘-加操作？

简答题
1.简述梯度下降法和牛顿法的优缺点？
2.正样本 10000，负样本 1000，怎样训练
3.Relu 相对于 sigmoid 函数的优缺点？
4.正则化方法？
5.说出物体检测、人脸识别、物体分割等某一领域的常见算法，并用一两句话简述其中一种算法的原理？

编程题
1.输入序列 a, 判断是否存在 i < j < k, 满足 a[i] < a[k] < a[j]，并写出算法复杂度？
2.输入多边形顶点坐标 List，判断是否为凸多边形(如果把一个多边形的所有边中，任意一条边向两方无限延长成为一直线时，其他各边都在此直线的同旁，那么这个多边形就叫做凸多边形)?

快手：
BN层解释
卷积时间复杂度
为什么要用softmax+交叉熵损失而不用softmax+均方根损失
一个dp的题：给了一堆数字，可以理解为股票交易，求最大的收益
百度

一面：
问项目，一直在问目标检测的时候出现一半的物体的处理
两个题：
判断镜像树
leetcode120（吐槽一下百度，一个做深度学习的有必要以刷题决定能否实习吗。。）
二面：
OpenCV，截取Mat矩阵的一部分区域的数据的具体实现，以及Mat内存管理的机制
一个期望的题 每次抽球概率是1/4，前三次抽中则第四次必中，求抽一百次的期望
给一个点和一个矩形（由长、宽和旋转角度表示），判断点是否在矩形中

因为较长 A:面试官    B:本人
A你是上午最后一个，咱们可以多聊一会（内心是崩溃的。。。）
A（原本以为要套路的自我介绍。。。结果。。。）你用C++多吗？
B不多，用java、python较多
A那STL熟悉吗？
B不熟
A知道Trie树吗？
B不熟，一顿扯
A详细讲解Trie树。
A红黑树了解吗？（据说让搞红黑树就要挂，难道这里就是预兆？？？）
B说了说5个特性
A详细讲解红黑树、B树、2-3-4树。
B（一脸谦虚的认真听）

A来做道题
一副扑克牌，未拆封，是有序的排列。要给4个人发牌，要使发的每一张牌的概率相同。
即发第i次牌，发出10和发出2的概率要相等。
B想了想，问了问，说了说思路，谈论了一下。

A排序算法知道哪些
B巴拉巴拉
A快排了解吗？
B说了思想，说了如何划分集合。
A知道快排的非递归实现吗？
B不了解
A那写个mergeSort吧，规定要写代码的。
B一会就搞定了

A咱们问问机器学习吧
A随机森林了解吗？Bagging和boosting了解吗？
B介绍随机森林
A RF的话，如果有一个特征和标签特别强相关。选择划分特征时，如果不随机的从所用特征中随机取一些特征的话，那么每一次那个强相关特征都会被选取。那么每个数都会是一样的。这就是随机森林随机选取一些特征的作用，让某些树，不选这个强相关特征。
B搜嘎。。。
A知道为什么bagging吗？
B。。。
A bootstrap aggregating
B又介绍了boosting

A说说这个项目吧
B巴拉巴拉
A看你项目用了SVM，介绍一下
B巴拉巴拉（中间被打断）
A你们怎么过来说的都很像啊，你们都看什么书？
B。。。我看的周志华的西瓜书和李航的统计学习方法。。。
A继续
B。。。巴拉巴拉
A还用到了RNN，介绍一下
B巴拉巴拉
链家
2017-08-27-链家-算法-内推1面-现场
来了之后先做1个小时的题，5道算法题

因为比较长，所以采用 A：面试官   B：本人

B 自我介绍

A 你自己学机器学习，怎么学的？
B 自己看书，周志华的西瓜书、机器学习实战。先找着撸代码，然后去深究里面的理论。

A西瓜书看到什么程度？
B刚开始看，看不太懂，然后就以机器学习实战为主，先照着撸代码，然后去西瓜书里深究里面的理论。

B我给您说说SVM吧，自学的时候留下很深的印象（试图抓住主动权~）
SVM是基于。。。说着手动起来写SVM的损失函数
A （打断）为什么样本点到决策面是 1/||w||
B 手推向量点到决策面的表达式（麻蛋，竟然一时紧张忘了。。。没推出来）
A 点到直线距离公式记得吧？
B 嗯嗯，又没写出来。只能说之前推过，现在一紧张忘了。。。
A 这个也无关紧要，继续
B 继续说SVM

A （打断）知道LR吧，知道LR和SVM有什么不同吗？
B 知道，首先这两个算法的分类思想不同，LR是基于概率推导的，SVM是基于最大化几何间隔的
A （打断）写一下，LR的损失函数
B 手写出来。其实这个sigmoid函数由那个什么族分布（真的忘了名字，其实是：指数族分布），加上二项分布导出来的。损失函数是由最大似然估计求出的。
A 怎么由最大似然估计导出的？推导一下
B 最大似然估计就是求让已知事件发生的概率最大的参数。
假设有5个样本，每一个的类别是yi，由LR计算出的概率是h(x)。那么每一个样本预测正确的概率为：
(H(x)^yi)*((1-h(x))^(1-yi)) ----
（刚开始一紧张，把h(x)和yi写反了）面试官说是这样吗？你这样全为0，我感觉你在背公式。。。你再看看
我一看，卧槽这竟然写错了。赶紧改过来，然后表明是自己紧张了。
概率连乘后，然后取对数就是LR的损失函数了。

A 为什么损失函数有个负号？
B 这是因为要应用梯度下降法，引入的。不加负号也可以，梯度上升法。这都是一样的。

A OK，继续，LR和SVM有什么区别？
B SVM决策面只由少量的支持向量决定，而LR的话是所有样本都会参与决策面的更新。
A 对，所以说SVM怎么样？
B SVM对于异常点不敏感，而LR敏感。SVM更加健壮，决策面不受非支持向量影响。
A OK

A 知道过拟合吧？
B 知道，在训练集表现好，在测试集表现一塌糊涂。举个例子就是：学生平时考试成绩非常棒，但一到实际应用就很烂。

A 说说常见的过拟合的解决办法
B 数据，样本不够，如果现在的训练集只是所有样本空间的一个小小的部分，那么这个模型的泛化能力就非常差（边画图，边说）
A 嗯嗯，还有呢
B 可以加正则项，L1，L2正则。L1还可以用来选择特征
A 为什么L1可以用来选择特征
B 因为L1的话会把某些不重要的特征压缩为0
A 为什么L1可以把某些特征压缩为0
B 因为（画图）L1约束是正方形的，经验损失最有可能和L1的正方形的顶点相交，L1比较有棱角。所以可以把某些特征压缩为0

A 还有什么过拟合的解决方法
B 神经网络中，dropout方法。就是每层网络的训练，随机的让一半神经元不工作。达到防止过拟合的目的

A 还有吗？
B 决策树中可以用剪枝操作。

B 决策树过拟合，可以用随机森林。。。
A 什么？？？现在一个决策树已经过拟合了，还要再以它为基准训练随机森林？
B 。。。对，你说的对。我想错了。。。
B 我就知道这些方法了。。。

A OK，挑一个项目给我说说吧
B 说项目（不记得中间有没有再提问了。。。）

B 要不我给您说说spark框架吧，之前还用的挺多。
A 嗯（看简历和笔试题中。。。）
B 开始说。。。说到三分之一

A 好了！ 你不必说了。（大手一挥~）我看你5道笔试题都没写思路，现在把第二题代码写出来
注： 第二题就是检测括号是否匹配
B 我写了啊。。。（给他翻到其中一个的背面）
A 哦，（迅速扫过代码，），为什要把字符压栈呢？不压栈也可以的。
B 是吗？{abc()}这样的也是合法的吗？
A 当然啊（看了一眼题。）
B 好吧，我本来也准备看到字符就丢到，不入栈。但担心这种情况不合法，就给入栈了。

A 嗯，第三题呢？
B 没思路，没写

A 给我说说第四题
第四题：10分钟内，恶意IP访问检测（10分钟内访问次数超过1024即为恶意访问）
B 这是10分钟动态检测的，需要时间刻度精确到秒吗？
A 不需要
B 把10分钟内的<ip,次数>存入hashmap, 再把key,value互换存入treemap。因为treemap是基于key有序的，升序。然后直接拿出来最后一个和1024比较。

A 怎么实现动态的检测，当前检测0-10分钟，那么第11分钟怎么办？
B 把0-10分钟的摘出来，从10分钟内的hashmap中减去，再把10-11分钟内的加上。
我知道这样实现起来，效率应该不高，但这一会我只想到了这个。。。
A 嗯，其实可以这样，把每分钟的分开存储，动态的向后移动，取这10个的总的数据就行。
甚至可以每分钟只存储TOP200的，然后10个分钟的汇总，取TOP1
B 嗯，明白了。

A 说说循环依赖这个怎么解决的？
第五题：系统有很多相互依赖的包，怎么检测循环依赖
B 把它当做一个链表。记录当前的名字在hashset中。如果某一次遍历的依赖名字存在于这个hashet中。就认为有循环依赖。

A 学过数据结构吧？学过图吧？给你一个有向图，怎么检测有环？
B 维护一个访问的数组，记录哪些点被访问过，从一点开始遍历，如果遍历的点被访问过，就说明有环
A 从哪个店开始遍历？
B 从入度为0的点开始遍历
A 如果有多个入度为0的点呢？
B 嗯。。。都要以它为入口开始遍历。
A show me the code!!!
（我内心是崩溃的。。。）
B 纠结了一会，又给他说了一遍思路。

A 嗯，好吧，我没有什么想问的了。你呢？
B 请问您说的这个图的这个应该怎么。。。算了，我还是下去自己看吧。。。我还是想知道怎么解决。。。
A 你说的对啊，就把思路给我讲了一下，和我的差不多。

B 贵公司这里机器学习、深度学习有什么应用场景呢？
A 房屋估价啊什么的。

B 好的，谢谢。再次感谢，离开。

-----------------------------------------------------------------------------------------------------------
2017-08-27-链家-算法-内推2面-现场
1、自我介绍
2、之前写过spark？写过统计日志用户数？那手写一下统计用户数（scala手写）
3、项目中用到了聚类？手写一下Kmeans
4、一般工业界不这样用，用kd-tree加速
5、给你出道题写一下，一个文件每一行有3列（\t分隔），每个字符串是abcd，这种形式，中间有大写有小写。
现认为：abcDe 等于BcaDe （即：不区分大小写，无关顺序）
要求输出： 字符 空格 出现次数 空格 每一种字符（以|分隔）
实例输出： abcde 2 abcDe|BcaDe
6、记不得了。。。好像没了。。。





更新线-2018-01-25 15:41---------------------------------------------------------------------------
转眼都到了2018年了。秋招都结束这么久了。最后拼多多也给了offer，不过太晚了，真奇怪这家公司，时间拖了那么久。
招聘结束后，不知道都忙了些啥，就到了这会儿。
前几天想编辑此贴，但被告知，加了精，没法编辑。好尴尬。刚才看到被解开了，这才过来编辑。
鉴于有许多人问关于招聘的一些问题。我就写一些招聘的一些事。包含一些经验之谈吧，用好了有奇效~~~在最后面~



-----------------------------------------------------------------------------------------------------------
阿里菜鸟-机器学习
2017-08-10--内推-1面-电话
没有自我介绍。。。直接略过。。。
1、讲一下你觉得你突出的地方，有亮点的地方。
说了SVM和LR
2、LR为什么用sigmoid函数。这个函数有什么优点和缺点？为什么不用其他函数？
3、SVM原问题和对偶问题关系？
4、KKT条件用哪些，完整描述
5、说项目
6、有一堆已经分好的词，如何去发现新的词？
面试官给的提示：用这个词和左右词的关系。互信息 新词的左右比较丰富，有的老词的左右也比较丰富。还要区分出新词和老词。
7、L1正则为什么可以把系数压缩成0，坐标下降法的具体实现细节
8、spark原理
9、spark Executor memory 给16G  executor core 给2个。问每个core分配多少内存

美丽联合
算法-内推1面-电话

1、自我介绍
2、介绍项目
3、说了SVM
4、为什么要把原问题转换为对偶问题？
因为原问题是凸二次规划问题，转换为对偶问题更加高效。
5、为什么求解对偶问题更加高效？
我答了，因为只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0.
6、alpha系数有多少个？
我答了：样本点的个数
7、避免过拟合的方法
答了：决策树剪枝、L2正则和L1正则
8、为什么L1正则可以实现参数稀疏，而L2正则不可以？
答了：L1正则因为是绝对值形式，很多系数被压缩为0,。而L2正则是很多系数被压迫到接近于0，而不是0
9、为什么L1很多系数可以被压缩为0，L2是被压缩至接近于0？
答了：图像上，L1正则是正方形，L2正则是圆形。
L1正则的往往取到正方形顶点，即有很多参数为0
L2正则往往去不到圆形和参数线的交点，即很多分量被压缩到接近于0

哪位大佬知道哪里有L1、L2的实现代码？？？，求告知~~~~~~~

10、问平时用啥语言比较多？
说了之前用java、scala多。现在用python较多。
11、问jvm 啥啥啥（没听清）。。。
答：不会
12、python...直接问你个开发中的实际问题吧，如果写的程序跑的非常慢，多方面分析这个问题？
答了： 1、检查程序是否有多层嵌套循环，优化
2、检查程序是否有很耗时的操作，看能否优化为多线程并行执行
3、检查数据量是否非常大，考虑是否可以用分布式计算模型。

求大佬补充~~~~~~~~~~
13、SQL中inner join 和outer join的区别？
14、试图给他说说SPARK，结果被严词拒绝（开玩笑的）。。。说时间紧迫，还是他来问吧。。。
15、Kmeans中，现在给你n个样本点不在欧式空间中，无法度量距离。现在给了一个函数F，可以衡量任意两个样本点的相似度。请问Kmeans如何操作？
答：想了一会，比如K=4的聚类。
1、首先，随机去4个点，作为初始类簇中心。
2、计算所有样本点与这4个点的F相似度。根据相似程度，把所有样本点分到4个类中。
3、在这4个类中，计算每一个样本点 i 到该类其他样本点的相似度和Si。取Si最大的那个点作为这个类的中心。
4、重复2、3步骤，直到类中心不再变化或者循环次数达到目标。

--------------------------------------------------------------------
2017-09-04-360-大数据算法-1面-内推-视频
1、自我介绍
2、说项目
3、说一下项目中用的Kmeans算法
4、知道哪几种聚类算法，说下原理
5、Kmeans有什么优缺点
6、项目用了RNN，说一下RNN原理
说了RNN原理，顺便说了LSTM/GRU的出现
7、为什么会出现长时依赖的问题
8、LSTM/GRU如何解决长时依赖的问题
9、写代码：
一个有序数组中查找某个数
一开始写了个遍历查找，面试官说，还能再快吗？
然后写了个二分查找

美团点评
2017-09-05--机器学习-1面-内推-电话
1、自我介绍
2、说项目
3、打断，问个扩展题：问答系统，有200W个FAQ，如何用分类模型做分类
思考ing，面试官提示：了解搜索引擎吗？
用倒排索引，把FAQ的问题分词，每个词对应多个FAQ。新来的query分词，每个词对应的FAQ拉出来。再在这个里面做分类。
4、继续说项目
5、说一下hadoop重要的2点
说shuffle，说map、reduce分别分配资源，可以细粒度控制资源占用情况，有利于超大任务平稳正常运行。
6、面试官说，其实是HDFS，正是由于有了分布式文件系统，才可以分布式计算
对，分布式文件系统。数据在哪里计算就在哪里，移动数据变成了移动计算。更高效
7、做题
给定二叉树前序、中序遍历结果。求后序遍历结果
8、一维空间中，2个线段，a1-b1 和a2-b2。判断是否两个线段有交集
他想要的答案是：一个线段里面的大坐标，小于等于另一个线段里面的小坐标。

2017-09-07-美团点评-机器学习-2面-内推-电话
1、自我介绍
2、说项目
3、用RNN了，说一下原理
说RNN，顺便说了长时依赖问题，介绍了LSTM，GRU
4、说情感分析的项目
5、每个句子都被打上标签正向或者负向情感，如果我想得出句子中的每个词的情感倾向，怎么做？
我不清楚该怎么做，就如下扯乎：
认为每个句子的情感倾向由每个词的情感倾向打分相加而得。
有的词正向：+1，+2，+3...
有的词负向：-1，-2，-3...
经过RNN，每一时刻的输出。。。扯完我现在想都想不通了。。。


后来想了想可以用贝叶斯分类。不知道对不对，还请大佬指正啊~~~~~
6、情感分析里用了SVM，说一下
说SVM，顺便跟LR对比一下
7、还知道其他分类算法吗
嗯嗯，知道，说了决策树，ID3，C4.5，再扯了扯bagging和boosting
8、做题
数轴上从左到右有n各点a[0], a[1], ……,a[n -1]，给定一根长度为L的绳子，求绳子最多能覆盖其中的几个点。要求时间复杂度O(n)，空间复杂度O(1)

-----------------------------------------------------------------------------------------------------------
2017-09-11-美团点评-机器学习-3面-内推-电话
1、自我介绍
2、说项目
3、场景题：一个景点有很多信息，位置、门票、类型等等。设计一个知识图谱。这个事情如果交给你来做，你会怎么推进
当时就一脸懵逼，只听过这个东西。没研究过。。。就硬着头皮瞎掰
4、我给介绍了SVM
5、你这机器学习这块，只学了这几个月。你认为你有什么优势能跟其他这个专业的人竞争？
麻蛋。。。确实没想过这个问题，继续瞎掰
6、又是场景题：有100亿网页，每个网页都有一个标签。有可能一个标签对应上百万标签，有的标签只对应几个标签。要做一个数据去重，每个标签只要1个网页。
7、工作中遇到了什么实际的难点问题，怎么解决的？

面试官是一个和蔼的秃顶大叔，估计是总监级别。问的问题就是有深度，考察解决问题能力

-----------------------------------------------------------------------------------------------------------
搜狗
2017-09-11--机器学习-1面-校招-现场
1、自我介绍
2、说项目
3、用RNN了，说一下原理
4、问RNN怎么训练的？
大概说了说，BPTT。这个不太懂
5、RNN的输入是什么呢？
有word2vec训练的词向量库，一个句子分词后，把词都换成对应的向量输入
6、继续说项目
7、项目用到聚类了？介绍一下
巴拉巴拉巴拉
8、说文本情感分类项目，文本向量用tf-idf这种有什么问题没有？
有，不能捕获到上下文之间的联系。以后尝试用doc2vec这种。
9、了解bagging和boosting吗？
巴拉巴拉
10、做题
1、全排列
2、数组第k大的数
3、数组左减右，求最大差
4、树的路径和


三星研究所
2017-09-19--机器学习-1 2 面-校招-现场
笔试：
早上笔试，一道题，3小时。。。
其实不是考编程，是考英语。。。
看题1小时，做题10分钟。。。
技术面试：
1、介绍项目
2、介绍RNN
3、Python如何定义一个私有变量
4、Java多线程start和run方法的区别
5、Java hashmap和hashtable的区别

西安三星电子研究所说有关机器学习的有存储SSD方面的、物联网平台方面的


猿辅导
作者：x.c.
链接：https://www.nowcoder.com/discuss/107126?type=0&order=0&pos=6&page=1
来源：牛客网

一面：
问了2.3个项目，因为我是嵌入式的，不怎么相关，解释了面试官也听不太懂
问go语言协程和线程的区别（不会）
问数据库的一个什么命令，我说只会增删查改。。
问http了解吗？（不太了解）
tcp-udp区别
tcp如何保持稳定传输（因为做项目做过，详细讲了下序列号，滑动窗口，拥塞控制）
算法题：“1+2-3213+3213。。。”算结果（做过）
算法题：“122233554884339”字符串去重（一开始用递归，写起来比较累，面试官提醒说可以用数据结构：二叉树，栈，队列里面某一个）
用栈很方便
提问：什么语言：java后台。。。C++可能用在视频上
你怎么看跨专业过来的？好好学习呗，我这边没问题，就带我而，面了

二面：
面试官貌似以前玩过单片机和STM32,问了问
你觉得你的计算机水平到什么阶段了？（本科毕业）
那你说一下虚函数和纯虚函数的区别（刚开始没记起来，提示了以下实例化，勉勉强强说出来了）
为什么构造函数不能用虚函数，为什么析构函数要用虚函数？（答得也不好，至此确定凉凉）
你了解STL吗？（了解）
map的实现机制是什么（红黑树）
为什么用红黑树（我没研究懂红黑树的原理，但是我知道树是为了更快地查找，AVL树是为了保持平均查找效率，红黑树是为了减少AVL树插入时的左旋右旋次数）
你这个怎么能说自己到达了本科毕业水平呢？（面试官笑嘻嘻的。。。）
算法题：前缀树的一道（不会）
那你怎么能说自己熟悉数据结构呢？（面试官笑嘻嘻）
算法题：剑指offer的题目，方阵找确定值。（刷过都忘了，提醒了一下才想起来）
上面哪个题目找最接近的值（又是一顿磕磕绊绊，最后写出来了）
提问：什么语言：java后台。。。
我可能对本科毕业水品有误解，我以后会努力的~~~谢谢您
知道自己凉了，不过面试体验的确很好，思路上一直在提醒。。。说面试体验很好，网上看都是大牛，（我们这边的确都是大牛）
好吧，再也不见~~~

招银网络
自我介绍，讲了一下项目；
问了机器学习和深度学习平时项目中哪个用的比较多；
先问了一下L1 和L2正则化的区别，可能是因为我简历里面写了SVM，对于SVM问的比较多，问了一下KT条件，对偶问题，核函数这些，问的很细，还有解决SVM的过拟合方法，最后扩展了一下别算法的解决过拟合问题；
C语言问了一个堆和栈的区别；
最后深度学习问了一个RNN模型适合解决什么问题，存在哪些问题；
还有一些记不太清了，整体感觉就是一直追问还有呢还有呢，最后说两周内出结果。


海康威视
技术面：
1）简单自我介绍
2）成绩如何？有成绩单可以看看吗？主要学了哪些课程？
3）sql中select *和select 列有什么区别?
4) 讲一下mysql中升序降序语句？（长时间没看忘记了，就没继续问sql了）
5）学习哪些语言？用过哪些做过项目？
6）谈谈你熟悉的项目？
7）项目中涉及到线程池，问：线程池具体如何使用？线程wait函数名称是什么？
8）项目中涉及到共享内存，问：共享内存如何使用，共享内存相关的函数有哪些？
9）项目中涉及消息队列，问:消息队列在项目中如何实现的，如果是你的话如何设计一个消息队列？
10）谈谈项目中遇到的最大的困难，如何解决的？

其它的暂时不记得了，面了30分钟的样子(面C++人比较多，每人约20-30分钟)，奇怪的是居然没问一点网络相关的内容。面完后门外等结果，两个不会两个答得模糊居然有幸过了技术面，过了几分钟直接被带去hr面。

hr面：
1）简单自我介绍一下
2）成绩如何？带成绩单没？
3）哪门成绩最好，哪门较差，为何较差？
4）你最大的几个缺点是什么，何时最烦恼？
5）介绍下你的家庭状况？
6）有女朋友？
7）你工作中遇到最大的困难，如何解决的？
8）你公司的加班情况如何？
9）不考虑留在本公司么？
10）看到我写的期望薪资后，问你已经拿到offer了么？
11）有几个企业可选择的话，你更看重公司哪些因素？
12）有什么想问的吗？

hr面结束就回去了，说1-2天会通知结果，全程没怎么谈薪资问题(可能是写的偏高)，事后想想感觉第一次面试大概就这样凉凉了！

农行软开
八位考官（人数记不太清，大概是八个）正对八位考生，不能报名字，只能报编号（匿名面试）。时间大概一小时多。之后会通知面试是否通过（第一批面试没有过的可以参加第二批面试）。
三个环节：
第一：每人一分钟的自我介绍，可以简单结合项目来介绍，要不然第三环节面试官没有东西可问（时间不要太长，否则面试官是会提醒的）。
第二：在屏幕上给一道情景题，给5分钟的时间让把答案写在纸上，然后挨个陈述答案，要把纸面向考官，中间考官可能会提问为什么，怎么做，如果有时间就把自己觉得有把握的答案写上，写关键点就可以，要不然时间不够。（数据库很重要）
第三：面试官随机向大家提问，其他人可能会被面试官点到补充答案，问题主要是根据你之前的自我介绍和项目，数据库方面的东西比较多（可能银行比较注重数据库），每个人的方向都不一（有人是机器学习，有人是Java，有人是c/c++）。
我觉得银行问的有广度，没深度。有些细节记不太清了，就胡乱写了一通。










